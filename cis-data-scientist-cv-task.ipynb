{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Download Dataset \nhttp://data.vision.ee.ethz.ch/cvl/food-101.tar.gz","metadata":{}},{"cell_type":"code","source":"# !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:46:13.015569Z","iopub.execute_input":"2025-03-31T15:46:13.015980Z","iopub.status.idle":"2025-03-31T15:46:13.020116Z","shell.execute_reply.started":"2025-03-31T15:46:13.015946Z","shell.execute_reply":"2025-03-31T15:46:13.019089Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import random\nimport numpy as np\nimport tensorflow as tf\nimport torch\nimport os\n\ndef set_global_seed(seed=42):\n    \"\"\"Sets seeds for reproducibility across multiple libraries.\"\"\"\n    random.seed(seed)  # Python random seed\n    np.random.seed(seed)  # NumPy random seed\n    tf.random.set_seed(seed)  # TensorFlow random seed\n    if torch is not None:\n        torch.manual_seed(seed)  # PyTorch seed for CPU\n        torch.cuda.manual_seed(seed)  # PyTorch seed for GPU\n        torch.cuda.manual_seed_all(seed)  # If using multiple GPUs\n        torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior\n        torch.backends.cudnn.benchmark = False  # Disables auto-optimization\n\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)  # Ensures hash-based operations are deterministic\n\n    print(f\"Global seed set to {seed}\")\n\n# Example usage:\nset_global_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:46:13.039797Z","iopub.execute_input":"2025-03-31T15:46:13.040048Z","iopub.status.idle":"2025-03-31T15:46:27.795343Z","shell.execute_reply.started":"2025-03-31T15:46:13.040027Z","shell.execute_reply":"2025-03-31T15:46:27.794570Z"}},"outputs":[{"name":"stdout","text":"Global seed set to 42\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport random\nimport tarfile\nimport shutil\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom PIL import Image\nfrom tqdm import tqdm\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import ResNet50, InceptionV3, EfficientNetB0\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom sklearn.model_selection import train_test_split\nimport requests","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:46:27.796355Z","iopub.execute_input":"2025-03-31T15:46:27.796895Z","iopub.status.idle":"2025-03-31T15:46:28.732042Z","shell.execute_reply.started":"2025-03-31T15:46:27.796871Z","shell.execute_reply":"2025-03-31T15:46:28.731400Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"!pip install --upgrade wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-31T15:46:28.733336Z","iopub.execute_input":"2025-03-31T15:46:28.733878Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /usr/local/lib/python3.10/dist-packages (0.19.1)\nCollecting wandb\n  Downloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (0.4.0)\nRequirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.1.43)\nRequirement already satisfied: platformdirs in /usr/local/lib/python3.10/dist-packages (from wandb) (4.3.6)\nRequirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\nRequirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\nRequirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.11.0a2)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.2)\nRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.32.3)\nRequirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.19.2)\nRequirement already satisfied: setproctitle in /usr/local/lib/python3.10/dist-packages (from wandb) (1.3.4)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (75.1.0)\nRequirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.10/dist-packages (from wandb) (4.12.2)\nRequirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=2.6->wandb) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2025.1.31)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nDownloading wandb-0.19.8-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.8/20.8 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: wandb\n  Attempting uninstall: wandb\n    Found existing installation: wandb 0.19.1\n    Uninstalling wandb-0.19.1:\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import wandb\n# from wandb.keras import WandbCallback\nfrom wandb.integration.keras import WandbCallback\nprint(wandb.__version__)\n# Initialize Weights & Biases","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wandb login 685a37da89a21c695a704814247055cc7112e6ad\n# wandb.init(project=\"food101_experiment_tracking\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_url = \"http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz\"\ndataset_path = \"food-101.tar.gz\"\nextract_folder = \"food-101\"\n\nif not os.path.exists(dataset_path):\n    print(\"Downloading dataset...\")\n    response = requests.get(dataset_url, stream=True)\n    with open(dataset_path, 'wb') as file:\n        shutil.copyfileobj(response.raw, file)\n    print(\"Download complete.\")\n\nif not os.path.exists(extract_folder):\n    print(\"Extracting dataset...\")\n    with tarfile.open(dataset_path, 'r:gz') as tar:\n        tar.extractall()\n    print(\"Extraction complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_dir = os.path.join(extract_folder, \"images\")\n!pwd\nprint(data_dir)\nall_classes = os.listdir(data_dir)\nrandom_classes = random.sample(all_classes, 25)\nprint(\"Selected Classes:\", random_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def organize_data(src_folder, dest_folder, classes, train_ratio=0.8):\n    if not os.path.exists(dest_folder):\n        os.makedirs(dest_folder)\n    \n    for cls in tqdm(classes, desc=\"Processing Classes\"):\n        class_path = os.path.join(src_folder, cls)\n        images = os.listdir(class_path)\n        train_images, test_images = train_test_split(images, train_size=train_ratio, random_state=42)\n        \n        for dataset, image_list in zip(['train', 'test'], [train_images, test_images]):\n            class_dest = os.path.join(dest_folder, dataset, cls)\n            os.makedirs(class_dest, exist_ok=True)\n            for img in image_list:\n                shutil.copy(os.path.join(class_path, img), class_dest)\n!rm -r /kaggle/working/food101_selected\norganized_data_path = \"food101_selected\"\norganize_data(data_dir, organized_data_path, random_classes)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_class_distribution(data_path):\n    class_counts = {cls: len(os.listdir(os.path.join(data_path, 'train', cls))) for cls in random_classes}\n    plt.figure(figsize=(12, 5))\n    sns.barplot(x=list(class_counts.keys()), y=list(class_counts.values()))\n    plt.xticks(rotation=90)\n    plt.title(\"Class Distribution\")\n    plt.show()\n\nplot_class_distribution(organized_data_path)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# data_gen = ImageDataGenerator(rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, fill_mode='nearest',shear_range=0.2,zoom_range=0.2, \n#                               horizontal_flip=True, validation_split=0.2)\n\n# train_gen = data_gen.flow_from_directory(os.path.join(organized_data_path, 'train'), target_size=(224, 224),\n#                                          batch_size=32, class_mode='categorical', subset='training')\n# val_gen = data_gen.flow_from_directory(os.path.join(organized_data_path, 'train'), target_size=(224, 224),\n#                                        batch_size=32, class_mode='categorical', subset='validation')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def build_model(base_model):\n#     # base_model.trainable = False  # Freeze base layers\n#     base_model.trainable = True  # Unfreeze all layers\n#     for layer in base_model.layers[:-20]:  # Keep first 20 layers frozen\n#         layer.trainable = False\n#     # model = Sequential([\n#     #     base_model,\n#     #     GlobalAveragePooling2D(),\n#     #     Dense(256, activation='relu'),\n#     #     Dropout(0.5),\n#     #     Dense(len(random_classes), activation='softmax')\n#     # ])\n#     model = Sequential([\n#         base_model,\n#         GlobalAveragePooling2D(),\n#         BatchNormalization(),\n#         Dense(256, activation='relu'),\n#         Dropout(0.5),\n#         Dense(len(random_classes), activation='softmax')\n#     ])\n#     return model\n\n# models = {\n#     \"ResNet50\": build_model(ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n#     \"InceptionV3\": build_model(InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n#     \"EfficientNetB0\": build_model(EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3)))\n# }","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def train_model(model, model_name, optimizer, lr):\n#     # model.compile(optimizer=optimizer(lr), loss='categorical_crossentropy', metrics=['accuracy'])\n#     model.compile(optimizer=optimizer(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n#     history = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=1, callbacks=[WandbCallback()])\n#     model.save(f\"{model_name}.h5\")\n#     return history\n\n\nfrom wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n\n# def train_model(model, model_name, optimizer, lr):\n#     model.compile(optimizer=optimizer(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n#     callbacks = [\n#         WandbMetricsLogger(),  # Tracks metrics in WandB\n#         WandbModelCheckpoint(f\"{model_name}.h5\", save_weights_only=True)  # Saves best model\n#     ]\n    \n#     history = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=1, callbacks=callbacks)\n#     return history\n\n# def train_model(model, model_name, optimizer, lr):\n#     model.compile(optimizer=optimizer(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    \n#     callbacks = [\n#         WandbMetricsLogger(),  # Tracks metrics in WandB\n#         WandbModelCheckpoint(f\"{model_name}.weights.h5\", save_weights_only=True)  # Fix file name issue\n#     ]\n    \n#     history = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=1, callbacks=callbacks)\n#     return history\n\n# optimizers = [Adam, RMSprop, SGD]\n# lrs = [0.001, 0.0005, 0.0001]\n\n# for model_name, model in models.items():\n#     for opt in optimizers:\n#         for lr in lrs:\n#             print(f\"Training {model_name} with {opt.__name__} and learning rate {lr}\")\n#             history = train_model(model, f\"{model_name}_{opt.__name__}_{lr}\", opt, lr)\n\n\n# from tensorflow.keras.models import clone_model\n\n# for model_name, model in models.items():\n#     for opt in optimizers:\n#         for lr in lrs:\n#             model_copy = clone_model(model)  # Create a fresh copy of the model\n#             model_copy.set_weights(model.get_weights())  # Copy original weights\n            \n#             print(f\"Training {model_name} with {opt.__name__} and learning rate {lr}\")\n#             history = train_model(model_copy, f\"{model_name}_{opt.__name__}_{lr}\", opt, lr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from tensorflow.keras.applications import ResNet50, InceptionV3, EfficientNetB0\n# from tensorflow.keras.models import Sequential\n# from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n# from tensorflow.keras.optimizers import Adam, RMSprop, SGD\n# from tensorflow.keras.models import clone_model\n# import wandb\n# from tensorflow.keras.callbacks import ReduceLROnPlateau\n# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n# from tensorflow.keras.callbacks import EarlyStopping\n# from tensorflow.keras.regularizers import l2\n# # from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n# from wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\n# lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n\n\n# # Function to clean GPU memory\n# def clean_gpu():\n#     tf.keras.backend.clear_session()  # Clears TensorFlow session\n#     gc.collect()  # Collects garbage\n#     try:\n#         tf.config.experimental.reset_memory_stats(\"GPU:0\")  # Clears GPU memory stats\n#     except:\n#         pass\n\n# # Build model function with trainable layers handling\n# def build_model(base_model, trainable_layers=10):\n#     base_model.trainable = True\n#     trainable_layers = min(trainable_layers, len(base_model.layers))  # Prevent out-of-bounds error\n\n#     for layer in base_model.layers[:-trainable_layers]:  \n#         layer.trainable = False  # Freeze initial layers\n\n#     model = Sequential([\n#         base_model,\n#         GlobalAveragePooling2D(),\n#         BatchNormalization(),\n#         Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n#         Dropout(0.6),\n#         Dense(train_gen.num_classes, activation='softmax') \n#     ])\n#     return model\n\n\n# # Initialize models\n# models = {\n#     \"ResNet50\": build_model(ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n#     \"InceptionV3\": build_model(InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n#     \"EfficientNetB0\": build_model(EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3)))\n# }\n\n\n# # Training function\n# def train_model(model, model_name, optimizer, lr):\n#     optimizer_instance = optimizer(learning_rate=lr)\n#     model.compile(optimizer=optimizer_instance, loss='categorical_crossentropy', metrics=['accuracy',\"f1_score\",])\n\n#     optimizer_name = optimizer_instance.__class__.__name__  # Get correct class name\n#     early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n#     callbacks = [\n#         WandbMetricsLogger(),\n#         WandbModelCheckpoint(f\"{model_name}_{optimizer_name}_{lr}.weights.h5\", save_weights_only=True)  # Ensure unique names\n#     ]\n\n#     history = model.fit(train_gen, validation_data=val_gen, epochs=3, verbose=1, callbacks=callbacks)\n\n#     del model, optimizer_instance\n#     clean_gpu()\n#     return history\n\n\n# # Optimizers and learning rates\n# optimizers = [Adam, RMSprop, SGD]\n# lrs = [0.001, 0.0005, 0.0001]\n\n# # Loop through models, optimizers, and learning rates\n# for model_name, _ in models.items():\n#     for opt in optimizers:\n#         for lr in lrs:\n#             model = build_model(eval(model_name)(weights='imagenet', include_top=False, input_shape=(224, 224, 3)))\n\n#             print(f\"Training {model_name} with {opt.__name__} and learning rate {lr}\")\n#             history = train_model(model, f\"{model_name}_{opt.__name__}_{lr}\", opt, lr)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# def build_model(base_model, trainable_layers=20):\n#     base_model.trainable = True\n#     trainable_layers = min(trainable_layers, len(base_model.layers))  # Prevent index error\n#     for layer in base_model.layers[:-trainable_layers]:  # Freeze only the required layers\n#         layer.trainable = False\n\n#     model = Sequential([\n#         base_model,\n#         GlobalAveragePooling2D(),\n#         BatchNormalization(),\n#         Dense(256, activation='relu'),\n#         Dropout(0.5),\n#         Dense(len(random_classes), activation='softmax')\n#         Dense(train_gen.num_classes, activation='softmax')\n#     ])\n#     return model\n    \n# models = {\n#     \"ResNet50\": build_model(ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n#     \"InceptionV3\": build_model(InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))),\n#     \"EfficientNetB0\": build_model(EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3)))\n# }\n\n# # def train_model(model, model_name, optimizer, lr):\n# #     optimizer_instance = optimizer(learning_rate=lr)  # Ensure fresh optimizer\n# #     model.compile(optimizer=optimizer_instance, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n# #     callbacks = [\n# #         WandbMetricsLogger(),\n# #         WandbModelCheckpoint(f\"{model_name}_{optimizer.__name__}_{lr}.weights.h5\", save_weights_only=True)  # Unique file names\n# #     ]\n    \n# #     history = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=1, callbacks=callbacks)\n# #     return history\n\n\n# def train_model(model, model_name, optimizer, lr):\n#     optimizer_instance = optimizer(learning_rate=lr)  # Ensure fresh optimizer\n#     model.compile(optimizer=optimizer_instance, loss='categorical_crossentropy', metrics=['accuracy'])\n\n#     optimizer_name = optimizer_instance.__class__.__name__  # Get class name safely\n\n#     callbacks = [\n#         WandbMetricsLogger(),\n#         WandbModelCheckpoint(f\"{model_name}_{optimizer_name}_{lr}.weights.h5\", save_weights_only=True)\n#     ]\n\n#     history = model.fit(train_gen, validation_data=val_gen, epochs=10, verbose=1, callbacks=callbacks)\n#     return history\n\n# # for model_name, model in models.items():\n# #     for opt in optimizers:\n# #         for lr in lrs:\n# #             model_copy = clone_model(model)  # Prevent weight contamination\n# #             model_copy.set_weights(model.get_weights())\n            \n# #             print(f\"Training {model_name} with {opt.__name__} and learning rate {lr}\")\n# #             history = train_model(model_copy, f\"{model_name}_{opt.__name__}_{lr}\", opt, lr)\n\n\n# # Instead of cloning, reinitialize model with proper pre-trained weights\n# for model_name, _ in models.items():\n#     for opt in optimizers:\n#         for lr in lrs:\n#             model = build_model(eval(model_name)(weights='imagenet', include_top=False, input_shape=(224, 224, 3)))  \n            \n#             print(f\"Training {model_name} with {opt.__name__} and learning rate {lr}\")\n#             history = train_model(model, f\"{model_name}_{opt.__name__}_{lr}\", opt, lr)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport gc\nimport tensorflow as tf\nfrom tensorflow.keras.applications import ResNet50, InceptionV3, EfficientNetB0\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.regularizers import l2\nfrom wandb.integration.keras import WandbMetricsLogger, WandbModelCheckpoint\nimport wandb\n\n# Initialize WandB\nwandb.init(project=\"CIS_image_classification\")\n\n# Function to clean GPU memory\ndef clean_gpu():\n    tf.keras.backend.clear_session()\n    gc.collect()\n    try:\n        tf.config.experimental.reset_memory_stats(\"GPU:0\")\n    except:\n        pass\n\n# Function to build model\ndef build_model(base_model, trainable_layers=10):\n    base_model.trainable = True\n    trainable_layers = min(trainable_layers, len(base_model.layers))\n    \n    for layer in base_model.layers[:-trainable_layers]:  \n        layer.trainable = False\n    \n    model = Sequential([\n        base_model,\n        GlobalAveragePooling2D(),\n        BatchNormalization(),\n        Dense(256, activation='relu', kernel_regularizer=l2(0.001)),\n        Dropout(0.6),\n        Dense(train_gen.num_classes, activation='softmax') \n    ])\n    return model\n\n# Data generators\ndata_gen = ImageDataGenerator(\n    rescale=1./255, rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, \n    shear_range=0.2, zoom_range=0.2, horizontal_flip=True, validation_split=0.2\n)\n\ntrain_gen = data_gen.flow_from_directory(\n    os.path.join(organized_data_path, 'train'), target_size=(224, 224),\n    batch_size=32, class_mode='categorical', subset='training'\n)\nval_gen = data_gen.flow_from_directory(\n    os.path.join(organized_data_path, 'train'), target_size=(224, 224),\n    batch_size=32, class_mode='categorical', subset='validation'\n)\n\n# Define models\nmodels = {\n    \"ResNet50\": ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n    \"InceptionV3\": InceptionV3(weights='imagenet', include_top=False, input_shape=(224, 224, 3)),\n    \"EfficientNetB0\": EfficientNetB0(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n}\n\n# Training function\ndef train_model(model, model_name, optimizer, lr):\n    optimizer_instance = optimizer(learning_rate=lr)\n    model.compile(optimizer=optimizer_instance, loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)\n    \n    callbacks = [\n        WandbMetricsLogger(),\n        WandbModelCheckpoint(f\"{model_name}_{optimizer.__name__}_{lr}.weights.h5\", save_weights_only=True),\n        early_stopping,\n        lr_scheduler\n    ]\n    \n    history = model.fit(train_gen, validation_data=val_gen, epochs=3, verbose=1, callbacks=callbacks)\n    \n    del model, optimizer_instance\n    clean_gpu()\n    return history\n\n# Optimizers and learning rates\noptimizers = [Adam, RMSprop, SGD]\nlrs = [0.001, 0.0005, 0.0001]\n\n# Train models with different optimizers and learning rates\nfor model_name, base_model in models.items():\n    for opt in optimizers:\n        for lr in lrs:\n            model = build_model(base_model)\n            print(f\"Training {model_name} with {opt.__name__} and learning rate {lr}\")\n            history = train_model(model, f\"{model_name}_{opt.__name__}_{lr}\", opt, lr)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def plot_feature_maps(model, image_path):\n    img = Image.open(image_path).resize((224, 224))\n    img_array = np.expand_dims(np.array(img) / 255.0, axis=0)\n    layer_outputs = [layer.output for layer in model.layers if 'conv' in layer.name]\n    activation_model = Model(inputs=model.input, outputs=layer_outputs)\n    activations = activation_model.predict(img_array)\n    first_layer_activation = activations[-2]\n    plt.figure(figsize=(10, 10))\n    for i in range(16):\n        plt.subplot(4, 4, i + 1)\n        plt.imshow(first_layer_activation[0, :, :, i], cmap='viridis')\n        plt.axis('off')\n    plt.show()\n\nplot_feature_maps(models['ResNet50'], os.path.join(organized_data_path, 'train', random_classes[0], os.listdir(os.path.join(organized_data_path, 'train', random_classes[0]))[0]))\n\nprint(\"Pipeline Completed Successfully!\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}